# ðŸ“ Project Structure

Complete overview of the Stellar Light Curve Anomaly Detector project.

```
ExopatternNetV3/
â”‚
â”œâ”€â”€ ðŸ“„ README.md                    # Main documentation
â”œâ”€â”€ ðŸ“„ QUICKSTART.md                # Quick start guide (5 minutes)
â”œâ”€â”€ ðŸ“„ EXAMPLES.md                  # Detailed usage examples
â”œâ”€â”€ ðŸ“„ PROJECT_STRUCTURE.md         # This file
â”œâ”€â”€ ðŸ“„ requirements.txt             # Python dependencies
â”œâ”€â”€ ðŸ“„ setup.py                     # Installation script
â”œâ”€â”€ ðŸ“„ .gitignore                   # Git ignore rules
â”‚
â”œâ”€â”€ ðŸš€ run.sh                       # Linux/Mac launcher
â”œâ”€â”€ ðŸš€ run.bat                      # Windows launcher
â”œâ”€â”€ ðŸ”§ generate_sample_data.py      # Sample data generator
â”‚
â”œâ”€â”€ ðŸ”™ backend/                     # ML Backend
â”‚   â”œâ”€â”€ ðŸ“„ app.py                   # Flask API entry point
â”‚   â”‚
â”‚   â”œâ”€â”€ ðŸ“‚ api/                     # API Layer
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ routes.py               # REST API endpoints
â”‚   â”‚
â”‚   â”œâ”€â”€ ðŸ“‚ data/                    # Data Ingestion
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ loader.py               # FITS/CSV loader
â”‚   â”‚
â”‚   â”œâ”€â”€ ðŸ“‚ ml/                      # Machine Learning
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ preprocessing.py        # Feature extraction
â”‚   â”‚   â”œâ”€â”€ models.py               # Anomaly detection models
â”‚   â”‚   â””â”€â”€ training.py             # Training pipeline
â”‚   â”‚
â”‚   â”œâ”€â”€ ðŸ“‚ models/                  # Saved Models
â”‚   â”‚   â””â”€â”€ .gitkeep
â”‚   â”‚
â”‚   â””â”€â”€ ðŸ“‚ uploads/                 # Temporary uploads
â”‚       â””â”€â”€ .gitkeep
â”‚
â”œâ”€â”€ ðŸ–¼ï¸ frontend/                    # User Interface
â”‚   â””â”€â”€ app.py                      # Streamlit app
â”‚
â”œâ”€â”€ ðŸ“Š data/                        # Data Storage
â”‚   â””â”€â”€ samples/                    # Sample light curves
â”‚       â”œâ”€â”€ normal_star.csv/fits
â”‚       â”œâ”€â”€ exoplanet_transit.csv/fits
â”‚       â”œâ”€â”€ stellar_flares.csv/fits
â”‚       â”œâ”€â”€ noisy_outliers.csv/fits
â”‚       â””â”€â”€ complex_system.csv/fits
â”‚
â””â”€â”€ ðŸ““ notebooks/                   # Jupyter Notebooks (optional)
    â””â”€â”€ (analysis notebooks)
```

---

## ðŸ” Detailed Component Breakdown

### Backend Components

#### 1. **Data Layer** (`backend/data/`)

**loader.py** - Universal light curve loader
- Loads FITS files (Kepler/TESS format)
- Loads CSV files (flexible column detection)
- Validates and cleans data
- Extracts metadata
- Provides summary statistics

**Key Classes:**
- `LightCurveLoader` - Main data loading class

**Supported Formats:**
```python
# FITS: Kepler/TESS standard
TIME, FLUX, FLUX_ERR columns in LIGHTCURVE extension

# CSV: Flexible format
time,flux,flux_err
0.0,1000.5,2.1
...
```

#### 2. **ML Layer** (`backend/ml/`)

**preprocessing.py** - Data preprocessing and feature extraction
- Sigma clipping for outlier removal
- Flux normalization
- Gap filling (interpolation)
- Feature extraction from windows
- Smoothing (Savitzky-Golay)
- Periodogram computation

**Key Classes:**
- `LightCurvePreprocessor` - Preprocessing pipeline

**Features Extracted (per window):**
- Statistical: mean, std, median, skewness, kurtosis
- Variability: MAD, RMS, peak-to-peak
- Trend: slope, intercept
- Difference: rate of change

**models.py** - Anomaly detection models
- Isolation Forest algorithm
- Local Outlier Factor (LOF)
- Statistical threshold detection
- Transit event detection
- Ensemble methods

**Key Classes:**
- `AnomalyDetector` - Main detection class
- `EnsembleAnomalyDetector` - Advanced ensemble

**Detection Methods:**
1. **Window-based**: Sliding window feature analysis
2. **Point-based**: Z-score threshold detection
3. **Event-based**: Transit and flare detection

**training.py** - Model training pipeline
- Train from files
- Train from DataFrames
- Generate synthetic data
- Model persistence (save/load)
- Cross-validation support

**Key Classes:**
- `ModelTrainer` - Training pipeline

#### 3. **API Layer** (`backend/api/`)

**routes.py** - REST API endpoints

**Endpoints:**
```
GET  /health                 - Health check
POST /api/analyze            - Analyze light curve
POST /api/train              - Train on real data
POST /api/train/synthetic    - Train on synthetic data
POST /api/export             - Export results
```

**app.py** - Flask application entry point

---

### Frontend Components

#### **Streamlit App** (`frontend/app.py`)

**Features:**
- ðŸ“Š File upload (FITS/CSV)
- ðŸ” Real-time analysis
- ðŸ“ˆ Interactive visualizations
- ðŸŽ“ Model training interface
- ðŸ’¾ Results export
- âš™ï¸ Parameter tuning

**Tabs:**
1. **Analyze** - Upload and analyze light curves
2. **Train Model** - Train custom models
3. **Statistics** - Detailed statistics
4. **About** - Documentation

**Visualizations:**
- Main light curve plot with anomaly highlighting
- 4-panel analysis dashboard:
  - Light curve with anomalies
  - Anomaly score distribution
  - Flux distribution (normal vs anomaly)
  - Running average trend

---

### Utility Scripts

#### **generate_sample_data.py**

Generates synthetic light curves with:
- Normal stellar variability
- Exoplanet transits (periodic dips)
- Stellar flares (spikes)
- Random outliers
- Combined anomalies

**Usage:**
```bash
python generate_sample_data.py --output-dir data/samples --format both --n-samples 5
```

**Output:** 5 sample light curves in both CSV and FITS formats

#### **run.sh / run.bat**

Automated launcher scripts that:
1. Create virtual environment (if needed)
2. Install dependencies (if needed)
3. Generate sample data (if needed)
4. Launch Streamlit app

---

## ðŸ”„ Data Flow

### Analysis Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Upload File    â”‚
â”‚  (FITS/CSV)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LightCurve     â”‚
â”‚  Loader         â”‚
â”‚  â€¢ Parse file   â”‚
â”‚  â€¢ Validate     â”‚
â”‚  â€¢ Clean data   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Preprocessor   â”‚
â”‚  â€¢ Normalize    â”‚
â”‚  â€¢ Sigma clip   â”‚
â”‚  â€¢ Fill gaps    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Feature        â”‚
â”‚  Extraction     â”‚
â”‚  â€¢ Windows      â”‚
â”‚  â€¢ Statistics   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Anomaly        â”‚
â”‚  Detection      â”‚
â”‚  â€¢ IF           â”‚
â”‚  â€¢ LOF          â”‚
â”‚  â€¢ Statistical  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Visualization  â”‚
â”‚  & Results      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Training Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Training Data  â”‚
â”‚  (Multiple      â”‚
â”‚   light curves) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Preprocess     â”‚
â”‚  Each Curve     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Extract        â”‚
â”‚  Features       â”‚
â”‚  (All curves)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Train Models   â”‚
â”‚  â€¢ IF           â”‚
â”‚  â€¢ LOF          â”‚
â”‚  â€¢ Scaler       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Save Model     â”‚
â”‚  (Persist)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ”‘ Key Technologies

### Backend
- **Python 3.8+** - Core language
- **NumPy** - Numerical computing
- **pandas** - Data manipulation
- **scikit-learn** - ML algorithms
- **astropy** - FITS file handling
- **scipy** - Scientific computing
- **Flask** - REST API
- **joblib** - Model persistence

### Frontend
- **Streamlit** - Web UI framework
- **Plotly** - Interactive plots

### Development
- **setuptools** - Packaging
- **venv** - Virtual environments

---

## ðŸ“¦ Dependencies

See `requirements.txt` for complete list:

```
# Core ML
numpy>=1.24.0
pandas>=2.0.0
scikit-learn>=1.3.0
scipy>=1.11.0

# Astronomy
astropy>=5.3.0

# Optional: Deep Learning
tensorflow>=2.13.0

# API
flask>=2.3.0
flask-cors>=4.0.0

# Frontend
streamlit>=1.28.0
plotly>=5.17.0

# Utilities
joblib>=1.3.0
requests>=2.31.0
pydantic>=2.0.0
```

---

## ðŸŽ¯ Design Principles

### 1. **Modularity**
- Clear separation of concerns
- Independent components
- Easy to extend

### 2. **Flexibility**
- Multiple file formats
- Multiple detection methods
- Configurable parameters

### 3. **User-Friendly**
- Simple interface
- Clear visualizations
- Helpful documentation

### 4. **Scientific Rigor**
- Validated algorithms
- Statistical methods
- Reproducible results

### 5. **Windows Compatibility**
- No Linux-specific dependencies
- Clear Windows instructions
- Batch file launcher

---

## ðŸš€ Extension Points

### Easy to Add:

1. **New File Formats**
   - Add parser to `loader.py`
   - Follow existing pattern

2. **New Features**
   - Add to `preprocessing.py`
   - Extend feature extraction

3. **New Detection Methods**
   - Add to `models.py`
   - Implement detector class

4. **New Visualizations**
   - Add to `frontend/app.py`
   - Use Plotly components

5. **New API Endpoints**
   - Add to `routes.py`
   - Follow REST conventions

---

## ðŸ“Š Performance Characteristics

### Typical Performance (on modern PC):

- **File Loading**: < 1 second
- **Preprocessing**: 1-2 seconds (2000 points)
- **Feature Extraction**: 2-5 seconds
- **Model Training**: 5-30 seconds (depending on data size)
- **Prediction**: < 1 second
- **Visualization**: Real-time (interactive)

### Memory Usage:

- **Baseline**: ~200 MB
- **Per Light Curve**: ~1-5 MB
- **Model**: ~10-50 MB

### Scalability:

- Light curves: Up to 100,000 points tested
- Training data: Up to 1000 light curves tested
- Concurrent users: 1-10 (not designed for high concurrency)

---

## ðŸ”’ Security Notes

For production deployment (not included in this local version):

- Add authentication to API
- Implement rate limiting
- Validate all uploads
- Sanitize file names
- Set upload size limits
- Use HTTPS
- Add CSRF protection

---

## ðŸ“ Code Quality

- Type hints in key functions
- Docstrings for all classes/methods
- Error handling throughout
- Logging at appropriate levels
- Input validation
- Clear variable names

---

For usage instructions, see:
- [README.md](README.md) - Complete documentation
- [QUICKSTART.md](QUICKSTART.md) - 5-minute guide
- [EXAMPLES.md](EXAMPLES.md) - Code examples
